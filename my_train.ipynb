{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import argparse\n",
    "from utils import *\n",
    "from torch.utils.data.dataset import *\n",
    "from torch.utils.data.sampler import *  #用于采样\n",
    "from torch.nn.utils.rnn import *\n",
    "import bisect\n",
    "from model import *\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size N] [--cuda]\n",
      "                             [--dropout DROPOUT] [--emb_dropout EMB_DROPOUT]\n",
      "                             [--clip CLIP] [--epochs EPOCHS]\n",
      "                             [--char_kernel_size CHAR_KERNEL_SIZE]\n",
      "                             [--word_kernel_size WORD_KERNEL_SIZE]\n",
      "                             [--emsize EMSIZE] [--char_layers CHAR_LAYERS]\n",
      "                             [--word_layers WORD_LAYERS]\n",
      "                             [--char_nhid CHAR_NHID] [--word_nhid WORD_NHID]\n",
      "                             [--log-interval N] [--lr LR] [--optim OPTIM]\n",
      "                             [--seed SEED] [--save SAVE] [--weight WEIGHT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\pc\\AppData\\Roaming\\jupyter\\runtime\\kernel-677ce300-c4b7-4e3e-b51a-d8cd9e39c393.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Joint Extraction of Entities and Relations\")\n",
    "parser.add_argument('--batch_size', type=int, default=32, metavar='N',\n",
    "                    help='batch size (default: 32)')\n",
    "parser.add_argument('--cuda', action='store_false',default=False,\n",
    "                    help='use CUDA (default: True)')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='dropout applied to layers (default: 0.5)')\n",
    "parser.add_argument('--emb_dropout', type=float, default=0.25,\n",
    "                    help='dropout applied to the embedded layer (default: 0.25)')\n",
    "parser.add_argument('--clip', type=float, default=0.35,\n",
    "                    help='gradient clip, -1 means no clip (default: 0.35)')\n",
    "parser.add_argument('--epochs', type=int, default=30,\n",
    "                    help='upper epoch limit (default: 30)')\n",
    "parser.add_argument('--char_kernel_size', type=int, default=3,\n",
    "                    help='character-level kernel size (default: 3)')\n",
    "parser.add_argument('--word_kernel_size', type=int, default=3,\n",
    "                    help='word-level kernel size (default: 3)')\n",
    "parser.add_argument('--emsize', type=int, default=50,\n",
    "                    help='size of character embeddings (default: 50)')\n",
    "parser.add_argument('--char_layers', type=int, default=3,\n",
    "                    help='# of character-level convolution layers (default: 3)')\n",
    "parser.add_argument('--word_layers', type=int, default=3,\n",
    "                    help='# of word-level convolution layers (default: 3)')\n",
    "parser.add_argument('--char_nhid', type=int, default=50,\n",
    "                    help='number of hidden units per character-level convolution layer (default: 50)')\n",
    "parser.add_argument('--word_nhid', type=int, default=300,\n",
    "                    help='number of hidden units per word-level convolution layer (default: 300)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='report interval (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=4,\n",
    "                    help='initial learning rate (default: 4)')\n",
    "parser.add_argument('--optim', type=str, default='SGD',\n",
    "                    help='optimizer type (default: SGD)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed (default: 1111)')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--weight', type=float, default=10.0,\n",
    "                    help='manual rescaling weight given to each tag except \"O\"')\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-14264340f678>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set the random seed manually for reproducibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"WARNING: You have a CUDA device, so you should probably run with --cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "print(args)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")#torch.device代表将torch.Tensor分配到的设备的对象\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "charset = Charset()#定义字符对象\n",
    "vocab = Vocabulary()#定义词汇对象\n",
    "vocab.load(\"data/myvocab.txt\")#读文件\n",
    "tag_set = Index()#定义对象\n",
    "tag_set.load(\"data/my_tag2id.txt\")#读文件  标记-id\n",
    "relation_labels = Index()#定义对象\n",
    "relation_labels.load('data/my_relation_labels.txt')#读文件   关系标签\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##这里导入的是处理过后的训练集和测试集.pk\n",
    "train_data = load('data/my_train.pk')\n",
    "#test_data = load('data/NYT_CoType/test.pk')\n",
    "\n",
    "#print(len(train_data[:500]))\n",
    "#print(type(train_data))\n",
    "#print(len(train_data),len(val_data),len(test_data))\n",
    "#train_data=train_data[:1000]\n",
    "#test_data=test_data[:100]\n",
    "val_size = int(0.02 * len(train_data))#设置验证集大小\n",
    "train_data, val_data = random_split(train_data, [len(train_data)-val_size, val_size])##torch里的函数 随机切分训练集和验证集\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(data, breakpoints):# 把数据集按照句子长度分组#data:包含三部分 句子中每个词的id 句子中每个词对应的char的id 句子对应的实体id\n",
    "    groups = [[] for _ in range(len(breakpoints)+1)]\n",
    "    for idx, item in enumerate(data):\n",
    "        i = bisect.bisect_left(breakpoints, len(item[0]))\n",
    "        groups[i].append(idx)\n",
    "    data_groups = [Subset(data, g) for g in groups]  #Subset是torch.utils.data.dataset中的\n",
    "    return data_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三个集合都按照句子的长度分了组\n",
    "train_data_groups = group(train_data, [60, 80, 100, 120, 140, 160])\n",
    "val_data_groups = group(val_data, [60, 80, 100, 120, 140, 160])\n",
    "#test_data_groups = group(test_data, [10, 20, 30, 40, 50, 60])\n",
    "#print(len(train_data_groups))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupBatchRandomSampler(object):##每一组的句子长度不同 分别建采样器进行随机采样\n",
    "    def __init__(self, data_groups, batch_size, drop_last):\n",
    "        self.batch_indices = []\n",
    "        for data_group in data_groups:\n",
    "            self.batch_indices.extend(list(BatchSampler(SubsetRandomSampler(data_group.indices),\n",
    "                                                        batch_size, drop_last=drop_last)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.batch_indices[i] for i in torch.randperm(len(self.batch_indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_indices, data):  #分batch   data有三部分   返回三部分的batch\n",
    "    #print(batch_indices,len(data))\n",
    "    batch = [data[idx] for idx in batch_indices]\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, tokens, tags = zip(*sorted_batch)\n",
    "\n",
    "    padded_sentences, lengths = pad_packed_sequence(pack_sequence([torch.LongTensor(_) for _ in sentences]),  #文本长度对齐\n",
    "                                                    batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    padded_tokens, _ = pad_packed_sequence(pack_sequence([torch.LongTensor(_) for _ in tokens]),\n",
    "                                           batch_first=True, padding_value=charset[\"<pad>\"])\n",
    "    padded_tags, _ = pad_packed_sequence(pack_sequence([torch.LongTensor(_) for _ in tags]),\n",
    "                                         batch_first=True, padding_value=tag_set[\"O\"])\n",
    "\n",
    "    return padded_sentences.to(device), padded_tokens.to(device), padded_tags.to(device), lengths.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.tensor(np.load(\"data/NYT_CoType/word2vec.vectors.npy\"))      #将词向量转换为张量 47463*300\n",
    "word_embedding_size = word_embeddings.size(1)\n",
    "pad_embedding = torch.empty(1, word_embedding_size).uniform_(-0.5, 0.5)  #随机初始化 1*word_embedding_size 大小的张量 取值在-0.5~0.5之间\n",
    "unk_embedding = torch.empty(1, word_embedding_size).uniform_(-0.5, 0.5)\n",
    "word_embeddings = torch.cat([pad_embedding, unk_embedding, word_embeddings])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###定义两个模型的层数和大小\n",
    "char_channels = [args.emsize] + [args.char_nhid] * args.char_layers\n",
    "word_channels = [word_embedding_size + args.char_nhid] + [args.word_nhid] * args.word_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"model.pt\"):      #是否存在已经构造好的模型\n",
    "    model=torch.load('model.pt')\n",
    "#——————————搭建模型————————————\n",
    "else:\n",
    "    model = Model(charset_size=len(charset), char_embedding_size=args.emsize, char_channels=char_channels,\n",
    "                  char_padding_idx=charset[\"<pad>\"], char_kernel_size=args.char_kernel_size, weight=word_embeddings,\n",
    "                  word_embedding_size=word_embedding_size, word_channels=word_channels,\n",
    "                  word_kernel_size=args.word_kernel_size, num_tag=len(tag_set), dropout=args.dropout,\n",
    "                  emb_dropout=args.emb_dropout).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [args.weight] * len(tag_set)   #生成一维向量 初始化值为arg.weight 维度为len(tag_set)\n",
    "weight[tag_set[\"O\"]] = 1     #\"O\"标签对应的权值为1\n",
    "weight = torch.tensor(weight).to(device)   #将weight转换为张量形式 device:分配到设备 这里是cpu\n",
    "criterion = nn.NLLLoss(weight, size_average=False)      #定义的损失函数\n",
    "optimizer = getattr(optim, args.optim)(model.parameters(), lr=args.lr)  #getattr 获取torch.optim对象的属性值 默认是SGD(随机梯度下降)   得到优化器的损失函数(模型参数)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()    #torch的模型训练\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    sampler = GroupBatchRandomSampler(train_data_groups, args.batch_size, drop_last=False)#采样器是定义的采样器对象 按组分batch分别采样\n",
    "\n",
    "    for idx, batch_indices in enumerate(sampler):\n",
    "        sentences, tokens, targets, lengths = get_batch(batch_indices, train_data) #分batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentences, tokens)  #词级别和字符级别建模\n",
    "        output = pack_padded_sequence(output, lengths, batch_first=True).data #文本长度对齐\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True).data #将标签的编码长度对齐\n",
    "        loss = criterion(output, targets)  #损失函数\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += len(targets)\n",
    "        if (idx+1) % args.log_interval == 0:\n",
    "            cur_loss = total_loss / count\n",
    "            elapsed = time.time() - start_time\n",
    "            percent = ((epoch-1)*len(sampler)+(idx+1))/(args.epochs*len(sampler))\n",
    "            remaining = elapsed / percent - elapsed\n",
    "            print(\"| Epoch {:2d}/{:2d} | Batch {:5d}/{:5d} | Elapsed Time {:s} | Remaining Time {:s} | \"\n",
    "                  \"lr {:4.2e} | Loss {:5.3f} |\".format(epoch, args.epochs, idx+1, len(sampler), time_display(elapsed),\n",
    "                                                       time_display(remaining), lr, cur_loss))\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_groups):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0.000000001\n",
    "    TP = 0.000000001\n",
    "    TP_FP = 0.000000001\n",
    "    TP_FN = 0.000000001\n",
    "    with torch.no_grad():\n",
    "        for batch_indices in GroupBatchRandomSampler(data_groups, args.batch_size, drop_last=False):\n",
    "            sentences, tokens, targets, lengths = get_batch(batch_indices, train_data)\n",
    "            output = model(sentences, tokens)\n",
    "            tp, tp_fp, tp_fn = measure(output, targets, lengths)\n",
    "            TP += tp\n",
    "            TP_FP += tp_fp\n",
    "            TP_FN += tp_fn\n",
    "            output = pack_padded_sequence(output, lengths, batch_first=True).data\n",
    "            targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            count += len(targets)\n",
    "            print(count,TP_FP,TP_FN)\n",
    "    return total_loss / count, TP/TP_FP, TP/TP_FN, 2*TP/(TP_FP+TP_FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(output, targets, lengths):\n",
    "    assert output.size(0) == targets.size(0) and targets.size(0) == lengths.size(0)\n",
    "    tp = 0\n",
    "    tp_fp = 0\n",
    "    tp_fn = 0\n",
    "    batch_size = output.size(0)\n",
    "    output = torch.argmax(output, dim=-1)\n",
    "    for i in range(batch_size):\n",
    "        length = lengths[i]\n",
    "        out = output[i][:length].tolist()\n",
    "        target = targets[i][:length].tolist()\n",
    "        out_triplets = get_triplets(out)\n",
    "        print(\"out:\",out_triplets)\n",
    "        tp_fp += len(out_triplets)\n",
    "        target_triplets = get_triplets(target)\n",
    "        print(\"target:\",target_triplets)\n",
    "        tp_fn += len(target_triplets)\n",
    "        for target_triplet in target_triplets:\n",
    "            for out_triplet in out_triplets:\n",
    "                if out_triplet == target_triplet:\n",
    "                    tp += 1\n",
    "    return tp, tp_fp, tp_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets(tags):\n",
    "    temp = {}\n",
    "    triplets = []\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if tag == tag_set[\"O\"]:\n",
    "            continue\n",
    "        pos, relation_label, role = tag_set[tag].split(\"-\")\n",
    "        if pos == \"B\" or pos == \"S\":\n",
    "            if relation_label not in temp:\n",
    "                temp[relation_label] = [[], []]\n",
    "            temp[relation_label][int(role) - 1].append(idx)\n",
    "    for relation_label in temp:\n",
    "        role1, role2 = temp[relation_label]\n",
    "        if role1 and role2:\n",
    "            len1, len2 = len(role1), len(role2)\n",
    "            if len1 > len2:\n",
    "                for e2 in role2:\n",
    "                    idx = np.argmin([abs(e2 - e1) for e1 in role1])\n",
    "                    e1 = role1[idx]\n",
    "                    triplets.append((e1, relation_label, e2))\n",
    "                    del role1[idx]\n",
    "            else:\n",
    "                for e1 in role1:\n",
    "                    idx = np.argmin([abs(e2 - e1) for e2 in role2])\n",
    "                    e2 = role2[idx]\n",
    "                    triplets.append((e1, relation_label, e2))\n",
    "                    del role2[idx]\n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    best_val_loss = None\n",
    "    lr = args.lr\n",
    "    all_val_loss = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_f1 = []\n",
    "\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(\"-\" * 118)\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train()\n",
    "            val_loss, precision, recall, f1 = evaluate(val_data_groups)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"-\" * 118)\n",
    "            print(\"| End of Epoch {:2d} | Elapsed Time {:s} | Validation Loss {:5.3f} | Precision {:5.3f} \"\n",
    "                  \"| Recall {:5.3f} | F1 {:5.3f} |\".format(epoch, time_display(elapsed),\n",
    "                                                           val_loss, precision, recall, f1))\n",
    "            print(\"-\" * 118)\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                with open(args.save, 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_val_loss = val_loss\n",
    "            else:\n",
    "                # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "                lr = lr / 4.0\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "            all_val_loss.append(val_loss)\n",
    "            all_precision.append(precision)\n",
    "            all_recall.append(recall)\n",
    "            all_f1.append(f1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 118)\n",
    "        print('Exiting from training early')\n",
    "\n",
    "    # Load the best saved model.\n",
    "    with open(args.save, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "\n",
    "    # Run on test data\n",
    "    test_loss, precision, recall, f1 = evaluate(test_data_groups)\n",
    "    print(\"=\" * 118)\n",
    "    print(\"| End of Training | Test Loss {:5.3f} | Precision {:5.3f} \"\n",
    "          \"| Recall {:5.3f} | F1 {:5.3f} |\".format(test_loss, precision, recall, f1))\n",
    "    print(\"=\" * 118)\n",
    "\n",
    "    with open(\"record.tsv\", \"wt\", encoding=\"utf-8\") as f:\n",
    "        for idx in range(len(all_val_loss)):\n",
    "            f.write(\"{:d}\\t{:5.3f}\\t{:5.3f}\\t{:5.3f}\\t{:5.3f}\\n\"\n",
    "                    .format(idx+1, all_val_loss[idx], all_precision[idx], all_recall[idx], all_f1[idx]))\n",
    "        f.write(\"\\n{:5.3f}\\t{:5.3f}\\t{:5.3f}\\t{:5.3f}\\n\".format(test_loss, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
