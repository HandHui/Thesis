{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定MAX_LENGTH ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "案件总数： 2201\n",
      "Counter({'provide_shelter_for': 835, 'sell_drugs_to': 472, 'posess': 470, 'traffic_in': 424})\n",
      "\n",
      "Counter({90: 29, 104: 25, 152: 24, 124: 22, 82: 22, 256: 20, 127: 20, 114: 19, 184: 19, 144: 19, 111: 18, 115: 18, 128: 17, 136: 17, 148: 17, 150: 17, 117: 17, 122: 17, 176: 17, 154: 16, 118: 16, 84: 16, 86: 16, 106: 16, 206: 16, 166: 16, 88: 16, 146: 16, 95: 16, 74: 15, 195: 15, 205: 15, 140: 15, 138: 15, 79: 15, 175: 15, 147: 15, 235: 15, 121: 15, 254: 14, 107: 14, 125: 14, 66: 14, 243: 14, 123: 14, 223: 14, 165: 14, 162: 13, 181: 13, 139: 13, 194: 13, 142: 13, 134: 13, 110: 13, 93: 13, 99: 13, 89: 13, 87: 12, 151: 12, 156: 12, 168: 12, 153: 12, 64: 12, 340: 12, 77: 12, 132: 12, 61: 12, 137: 11, 78: 11, 192: 11, 65: 11, 143: 11, 167: 11, 185: 11, 221: 11, 116: 11, 67: 11, 204: 11, 179: 10, 191: 10, 119: 10, 112: 10, 161: 10, 220: 10, 244: 10, 135: 10, 233: 10, 237: 10, 155: 10, 96: 10, 75: 10, 85: 10, 270: 10, 169: 10, 73: 10, 101: 9, 164: 9, 187: 9, 199: 9, 186: 9, 97: 9, 193: 9, 149: 9, 157: 9, 72: 9, 83: 9, 113: 9, 212: 9, 240: 9, 336: 9, 91: 8, 285: 8, 182: 8, 145: 8, 501: 8, 98: 8, 246: 8, 214: 8, 92: 8, 76: 8, 171: 8, 247: 8, 80: 8, 395: 8, 299: 7, 470: 7, 172: 7, 226: 7, 267: 7, 190: 7, 120: 7, 227: 7, 196: 7, 203: 7, 208: 7, 177: 7, 130: 7, 219: 7, 287: 7, 109: 7, 310: 7, 81: 7, 126: 6, 211: 6, 197: 6, 170: 6, 71: 6, 263: 6, 210: 6, 222: 6, 335: 6, 207: 6, 216: 6, 276: 6, 275: 6, 366: 6, 189: 6, 50: 6, 274: 6, 202: 6, 296: 6, 242: 6, 357: 5, 159: 5, 63: 5, 133: 5, 105: 5, 200: 5, 108: 5, 355: 5, 230: 5, 225: 5, 201: 5, 343: 5, 69: 5, 351: 5, 100: 5, 493: 5, 224: 4, 131: 4, 209: 4, 215: 4, 103: 4, 314: 4, 304: 4, 286: 4, 129: 4, 415: 4, 269: 4, 448: 4, 260: 4, 70: 4, 198: 4, 48: 4, 234: 4, 291: 4, 236: 4, 58: 4, 141: 3, 297: 3, 241: 3, 347: 3, 413: 3, 163: 3, 486: 3, 282: 3, 315: 3, 393: 3, 102: 3, 295: 3, 316: 3, 360: 3, 446: 3, 352: 3, 273: 3, 337: 3, 255: 3, 390: 3, 94: 3, 301: 3, 160: 3, 358: 3, 158: 3, 253: 3, 288: 3, 386: 3, 306: 3, 365: 3, 370: 3, 218: 3, 322: 3, 248: 3, 440: 3, 232: 3, 289: 3, 173: 3, 327: 3, 513: 3, 229: 3, 266: 2, 53: 2, 252: 2, 265: 2, 385: 2, 376: 2, 258: 2, 329: 2, 312: 2, 293: 2, 251: 2, 249: 2, 174: 2, 339: 2, 305: 2, 178: 2, 268: 2, 319: 2, 362: 2, 408: 2, 231: 2, 417: 2, 250: 2, 473: 2, 284: 2, 68: 2, 429: 2, 320: 2, 271: 2, 697: 2, 349: 2, 402: 2, 60: 1, 39: 1, 217: 1, 298: 1, 281: 1, 280: 1, 374: 1, 325: 1, 213: 1, 54: 1, 324: 1, 264: 1, 317: 1, 180: 1, 341: 1, 367: 1, 59: 1, 318: 1, 228: 1, 238: 1, 51: 1, 183: 1, 47: 1, 583: 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../bishe_e1/data/mytrain.json','r',encoding='gbk') as f2:\n",
    "    lines = f2.readlines()\n",
    "#     raw_tra = json.load(f2)\n",
    "f2.close()\n",
    "\n",
    "length = len(lines)\n",
    "\n",
    "label_list = []\n",
    "count_list = []\n",
    "for i in range(length):\n",
    "    sentence = json.loads(lines[i])\n",
    "    label = sentence['relationMentions'][0]['label']\n",
    "    label_list.append(label)\n",
    "    sent_con = sentence['sentText'].strip()\n",
    "    count_list.append(len(sent_con))\n",
    "    \n",
    "\n",
    "# sentence = json.loads(lines[10])\n",
    "print('案件总数：',length)\n",
    "assert len(label_list) == length\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(label_list))\n",
    "\n",
    "print()\n",
    "print(Counter(count_list))\n",
    "\n",
    "#### 可以确定每一个sentence max_length = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json --> pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 300#最大句长\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_set(tag_set, relation_label):##生成标签集合tag_set\n",
    "    if relation_label == \"None\":\n",
    "        return\n",
    "    for pos in \"BIES\":\n",
    "        for role in \"12\":\n",
    "            tag_set.add(\"-\".join([pos, relation_label, role]))#pos-relation_label-role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(charset)    ##2194\n",
    "# charset[-1]   ### '<unk>'\n",
    "#charset[-2]  ###'<pad>'\n",
    "#charset[2192] ### '<pad>'\n",
    "#charset[2193] ### '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_labels = Index()\n",
    "entity_labels = Index()\n",
    "tag_set = Index()\n",
    "\n",
    "tag_set.add(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tmp = 0\n",
    "# min_tmp = 100\n",
    "# max_span = 0\n",
    "\n",
    "# for line in lines:\n",
    "#     line = line.strip()\n",
    "#     line = json.loads(line)\n",
    "    \n",
    "#     e1 = int(line[\"relationMentions\"][0]['e1start'])\n",
    "#     e2 = int(line[\"relationMentions\"][0]['e21start'])\n",
    "    \n",
    "#     if e2 > 300 :\n",
    "#         print(e1)\n",
    "#         print(e2)\n",
    "#         print()\n",
    "#     if len(line[\"sentText\"].strip().strip('\"')) > 300:\n",
    "#         e1 = int(line[\"relationMentions\"][0]['e1start'])\n",
    "#         e2 = int(line[\"relationMentions\"][0]['e21start'])\n",
    "#         if e2-e1 > max_span:\n",
    "#             max_span = e2-e1\n",
    "        \n",
    "# #         if e2-e1 == 242:\n",
    "# #             print(line)\n",
    "        \n",
    "#         if e2 > max_tmp:\n",
    "#             max_tmp = e2\n",
    "            \n",
    "#         if e1 <min_tmp:\n",
    "#             min_tmp = e1\n",
    "            \n",
    "        \n",
    "#         if e2 > 350:\n",
    "#             print('Hello')\n",
    "#         if e2 > 256:\n",
    "#             print('World')\n",
    "#         print(line[\"sentText\"].strip().strip('\"'))\n",
    "#         print(line[\"relationMentions\"][0]['e1start'])\n",
    "#         print(line[\"relationMentions\"][0]['e21start'])\n",
    "# print(max_tmp,min_tmp,max_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Supdate_tag_seq(em_text, e_start, sentence_text, relation_label, role, tag_set, tags_idx):#给句子中的实体打标签 得到tags_idx\n",
    "    overlap = False\n",
    "    wrongstart=0\n",
    "    \n",
    "    start=e_start\n",
    "    if start==-1:\n",
    "        wrongstart+=1\n",
    "    tag = \"-\".join([\"S\", relation_label, str(role)])\n",
    "    if len(em_text) == 1:\n",
    "        #print(em_text,e_start)\n",
    "        if tags_idx[start] != tag_set[\"O\"]:\n",
    "            overlap = True\n",
    "        tags_idx[start] = tag_set[tag]#S-...-...\n",
    "    else:\n",
    "        tag = \"B\" + tag[1:]\n",
    "        if tags_idx[start] != tag_set[\"O\"]:\n",
    "            overlap = True\n",
    "        tags_idx[start] = tag_set[tag]\n",
    "        tag = \"E\" + tag[1:]\n",
    "        end = start + len(em_text) - 1 #计算实体词的长度\n",
    "        if tags_idx[end] != tag_set[\"O\"]:\n",
    "            overlap = True\n",
    "        tags_idx[end] = tag_set[tag]\n",
    "        tag = \"I\" + tag[1:]\n",
    "        for index in range(start + 1, end):\n",
    "            if tags_idx[index] != tag_set[\"O\"]:\n",
    "                overlap = True\n",
    "            tags_idx[index] = tag_set[tag]\n",
    "            \n",
    "    #print(\"wrong start:\",wrongstart)\n",
    "    return overlap              #返回重复的已标记过的实体\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_data_set(fin, charset, vocab, relation_labels, entity_labels, tag_set, train, fout)\n",
    "#参数：输入的.json文件，字符集对象，单词集合对象（已导入词典），关系标签，实体标签，实体-关系标签集合，train/test list，overlap.txt文件\n",
    "def prepare_data_set(fin, charset, relation_labels, entity_labels, tag_set, dataset, fout):##预处理数据集  json->.pk\n",
    "\n",
    "    num_overlap = 0\n",
    "    for line in fin:\n",
    "        overlap = False\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        for relation_mention in sentence[\"relationMentions\"]:\n",
    "            relation_labels.add(relation_mention[\"label\"])  #记录关系类型\n",
    "            make_tag_set(tag_set, relation_mention[\"label\"]) #生成标签集合 BIES-relation-12\n",
    "        for entity_mention in sentence[\"entityMentions\"]:\n",
    "            entity_labels.add(entity_mention[\"label\"])  #记录实体类型\n",
    "        \n",
    "        sentence_text=list((sentence[\"sentText\"].strip().strip('\"')))\n",
    "        \n",
    "        \n",
    "#         if length_sent > MAX_SENT_LENGTH:#设置了最大句长\n",
    "# #             continue\n",
    "#             sentence_text = sentence_text[50:350]\n",
    "        length_sent = len(sentence_text) #该条文本长度\n",
    "        \n",
    "        sentence_idx = prepare_sequence(sentence_text, charset)  #charset是词和id的映射 将sentence中的每个词转为对应的id值得到id的list\n",
    "        tags_idx = [tag_set[\"O\"]] * length_sent #length_sent是该句子的长度  初始化为全0\n",
    "        \n",
    "        for relation_mention in sentence[\"relationMentions\"]:\n",
    "            if relation_mention[\"label\"] == \"None\":\n",
    "                continue\n",
    "#             relation_label = relation_mention['label']\n",
    "            em1_text = list(relation_mention[\"em1Text\"])  #英文中 对phrase分词\n",
    "            e1_s=int(relation_mention[\"e1start\"])\n",
    "            if e1_s==-1:\n",
    "                wrongstart+=1\n",
    "#             tag = \"-\".join([\"S\", relation_label, str(role)])           \n",
    "            res1 = Supdate_tag_seq(em1_text, e1_s, sentence_text, relation_mention[\"label\"], 1, tag_set, tags_idx)#对句子中的实体打标签\n",
    "            em2_text = list(relation_mention[\"em2Text\"])\n",
    "            e2_s=int(relation_mention[\"e21start\"])\n",
    "            res2 = Supdate_tag_seq(em2_text, e2_s, sentence_text, relation_mention[\"label\"], 2, tag_set, tags_idx)\n",
    "            \n",
    "            if res1 or res2:\n",
    "                num_overlap += 1 #重复\n",
    "                overlap = True\n",
    "#         print(tags_idx)\n",
    "        \n",
    "        \n",
    "        dataset.append((sentence_idx, tags_idx)) #dateset是train/test  包含三部分 句子中每个词的id 句子中每个词对应的char的id 句子对应的实体id        \n",
    "        fout.write(''.join(sentence_text)+'\\n')\n",
    "#         if overlap:\n",
    "#             fout.write(line+\"\\n\")\n",
    "            \n",
    "#         break\n",
    "    return num_overlap  #返回重复的已标记过的语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin...\n",
      "over...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Chinesecorpus=open('../bishe_e1/data/mycorpus.txt','rt',encoding='utf-8').read()\n",
    "cs_list = list(Chinesecorpus)\n",
    "cs_set_list = list(set(cs_list))\n",
    "cs_set_list.sort(key = cs_list.index)\n",
    "corpus_char=''.join(cs_set_list)    ###2192\n",
    "charset=ChineseCharset(corpus_char)\n",
    "\n",
    "relation_labels = Index()\n",
    "entity_labels = Index()\n",
    "tag_set = Index()\n",
    "tag_set.add(\"O\")\n",
    "\n",
    "with open(\"../data/my_train.txt\", \"wt\", encoding=\"utf-8\") as fout:\n",
    "    train = []\n",
    "#     train_seg=[]\n",
    "#     with open(r'../data/mytrain_seg.txt','wt',encoding='utf-8') as fseg:\n",
    "    with open('../bishe_e1/data/mytrain.json', 'rt', encoding='utf-8') as fin:##对训练集进行预处理\n",
    "\n",
    "        res = prepare_data_set(fin, charset,relation_labels, entity_labels, tag_set, train, fout)\n",
    "\n",
    "    save(train, '../data/my_train.pk')\n",
    "        \n",
    "#         fseg.write('\\n'.join(train_seg))\n",
    "    print('begin...')\n",
    "with open(\"../data/my_test.txt\", \"wt\", encoding=\"utf-8\") as fout:\n",
    "    test=[]\n",
    "\n",
    "    with open(r'..\\bishe_e1\\data\\mytest.json', 'rt', encoding='utf-8') as fin:##对训练集进行预处理\n",
    "#             res = prepare_data_set(fin, charset, vocab, relation_labels, entity_labels, tag_set, test, fout,test_seg)##参数：输入的.json文件，字符集对象，单词集合对象，关系标签，实体标签，实体-关系标签集合，train/test list，overlap.txt文件\n",
    "        res = prepare_data_set(fin, charset,relation_labels, entity_labels, tag_set, test, fout)\n",
    "    save(test, '../data/my_test.pk')\n",
    "#         fseg.write('\\n'.join(test_seg))\n",
    "\n",
    "    print('over...')\n",
    "relation_labels.save('../data/my_relation_labels.txt')\n",
    "entity_labels.save('../data/my_entity_labels.txt')\n",
    "tag_set.save(\"../data/my_tag2id.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试正确性 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'核'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chinesecorpus=open('../bishe_e1/data/mycorpus.txt','rt',encoding='utf-8').read()\n",
    "cs_list = list(Chinesecorpus)\n",
    "cs_set_list = list(set(cs_list))\n",
    "cs_set_list.sort(key = cs_list.index)\n",
    "corpus_char=''.join(cs_set_list)    ###2192\n",
    "charset=ChineseCharset(corpus_char)\n",
    "charset[1186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2201\n",
      "74\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/my_train.pk', 'rb') as f1:\n",
    "    train_data = pickle.load(f1)\n",
    "f1.close()\n",
    "\n",
    "print(len(train_data)) ## 2201\n",
    "print(len(train_data[0][0])) ### 74\n",
    "print(len(train_data[0][1])) ### 74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'诉'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 6,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 23,\n",
       " 27,\n",
       " 28,\n",
       " 23,\n",
       " 27,\n",
       " 29,\n",
       " 23,\n",
       " 23,\n",
       " 27,\n",
       " 30,\n",
       " 23,\n",
       " 27,\n",
       " 31,\n",
       " 23,\n",
       " 23,\n",
       " 32,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 33,\n",
       " 15,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 6,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 14,\n",
       " 40,\n",
       " 41,\n",
       " 39,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n",
      "170\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "# with open('../data/my_test.pk', 'rb') as f1:\n",
    "#     train_data = pickle.load(f1)\n",
    "# f1.close()\n",
    "\n",
    "# print(len(train_data)) ## 550\n",
    "# print(len(train_data[0][0])) ### 170\n",
    "# print(len(train_data[0][1])) ### 170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简介 ###\n",
    "上面为正式的预处理<br>\n",
    "下面是做出上述预处理的一些测试程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../bishe_e1/data/mytrain.json', 'rt', encoding='utf-8') as fin:##对训练集进行预处理\n",
    "    lines = fin.readlines()\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = lines[0].strip()\n",
    "sentence = json.loads(line)\n",
    "\n",
    "#### 涉及多个关系时，这样做比较合理\n",
    "for relation_mention in sentence[\"relationMentions\"]:\n",
    "    relation_labels.add(relation_mention[\"label\"])  #记录关系类型\n",
    "    make_tag_set(tag_set, relation_mention[\"label\"]) #生成标签集合 BIES-relation-12\n",
    "for entity_mention in sentence[\"entityMentions\"]:\n",
    "    entity_labels.add(entity_mention[\"label\"])  #记录实体类型\n",
    "#sentence_text = sentence[\"sentText\"].strip().strip('\"').split()#保存文本为单词list\n",
    "sentence_text=list((sentence[\"sentText\"].strip().strip('\"')))\n",
    "\n",
    "length_sent = len(sentence_text) #该条文本长度\n",
    "\n",
    "if length_sent > MAX_SENT_LENGTH:#设置了最大句长\n",
    "    print(\"Too Long !!!\")\n",
    "#     continue\n",
    "\n",
    "sentence_idx = prepare_sequence(sentence_text, charset)\n",
    "\n",
    "tags_idx = [tag_set[\"O\"]] * length_sent #length_sent是该句子的长度  初始化为全0\n",
    "\n",
    "relation_mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_idx):\n",
    "    return [to_idx[key] for key in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "25\n",
      "26\n",
      "27\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(tags_idx)\n",
    "for i in range(len(tags_idx)):\n",
    "    tag = tags_idx[i]\n",
    "    if tag != 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-provide_shelter_for-1\n"
     ]
    }
   ],
   "source": [
    "wrongstart = 0\n",
    "relation_label = relation_mention[\"label\"]\n",
    "role = 1\n",
    "for relation_mention in sentence[\"relationMentions\"]:\n",
    "    if relation_mention[\"label\"] == \"None\":\n",
    "        continue\n",
    "    em1_text = list(relation_mention[\"em1Text\"])  #英文中 对phrase分词\n",
    "    e1_s=int(relation_mention[\"e1start\"])\n",
    "    if e1_s==-1:\n",
    "        wrongstart+=1\n",
    "    tag = \"-\".join([\"S\", relation_label, str(role)])\n",
    "    print(tag)\n",
    "#     print(tag_set[14])\n",
    "    \n",
    "    assert len(tags_idx) == len(sentence_text)\n",
    "    res1 = Supdate_tag_seq(em1_text, e1_s, sentence_text, relation_mention[\"label\"], 1, tag_set, tags_idx)#对句子中的实体打标签\n",
    "    \n",
    "    em2_text = list(relation_mention[\"em2Text\"])\n",
    "    e2_s=int(relation_mention[\"e21start\"])\n",
    "    res2 = Supdate_tag_seq(em2_text, e2_s, sentence_text, relation_mention[\"label\"], 2, tag_set, tags_idx)\n",
    "#     if res1 or res2:\n",
    "#         num_overlap += 1 #重复\n",
    "#         overlap = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-provide_shelter_for-1', 'B-provide_shelter_for-2')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set[1],tag_set[2]\n",
    "\n",
    "tag_set[3],tag_set[4]\n",
    "\n",
    "tag_set[5],tag_set[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "25\n",
      "26\n",
      "27\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(tags_idx)\n",
    "for i in range(len(tags_idx)):\n",
    "    tag = tags_idx[i]\n",
    "    if tag != 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公', '诉', '机', '关', '指', '控', '，', '2', '0', '1', '5', '年', '1', '1', '月', '份', '的', '一', '天', '晚', '上', '，', '被', '告', '人', '简', '某', '某', '容', '留', '陈', '某', '、', '吴', '某', '、', '杨', '某', '某', '、', '张', '某', '、', '伍', '某', '某', '在', '简', '某', '某', '家', '一', '楼', '卧', '室', '，', '以', '烫', '吸', '的', '方', '式', '吸', '食', '甲', '基', '苯', '丙', '胺', '（', '冰', '毒', '）', '。']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
