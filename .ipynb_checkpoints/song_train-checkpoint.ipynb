{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinesecorpus=open('../bishe_e1/data/mycorpus.txt','rt',encoding='utf-8').read()\n",
    "cs_list = list(Chinesecorpus)\n",
    "cs_set_list = list(set(cs_list))\n",
    "cs_set_list.sort(key = cs_list.index)\n",
    "corpus_char=''.join(cs_set_list)   \n",
    "charset=ChineseCharset(corpus_char)            ### 2192 +'<UNK>'+<PAD>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/词向量/totalcase_256d.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines();\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for i in charset:\n",
    "#     if i in w2v_vocab:\n",
    "#         count += 1\n",
    "# print(count/len(charset))   ## 0.9708295350957156     256词向量中覆盖的字数\n",
    " \n",
    "# print(count,len(charset))   ## 2130 2194             前者是被覆盖的数，后者是字表长度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key2id = {}\n",
    "id2vector = []\n",
    "index = 0\n",
    "for line in lines[1:]:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ',1)\n",
    "    id2vector.append(line[1].split(' '))\n",
    "    key2id[line[0]] = index\n",
    "    index += 1\n",
    "#     w2v_vocab.append(line[0])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key2id) == len(id2vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "char_embedding = []\n",
    "for i in charset:\n",
    "    index = key2id.get(i,-1)\n",
    "    if index == -1:\n",
    "        vector = np.random.rand(256)\n",
    "    else:\n",
    "#         vector = id2vector[index]\n",
    "        vector = np.array(id2vector[index],dtype='float32')\n",
    "        count += 1\n",
    "        \n",
    "    char_embedding.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/char_embedding.npy',char_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('../data/char_embedding.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2194, 256])\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from torch.utils.data.dataset import *\n",
    "from torch.utils.data.sampler import *  #用于采样\n",
    "from torch.nn.utils.rnn import *\n",
    "import bisect\n",
    "from model import *\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Joint Extraction of Entities and Relations\")\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "emb_dropout = 0.25\n",
    "\n",
    "clip = 0.35\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "char_kernel_size = 3\n",
    "\n",
    "word_kernel_size = 3\n",
    "\n",
    "emsize = 50\n",
    "\n",
    "char_layers = 3\n",
    "\n",
    "word_layers = 3\n",
    "\n",
    "char_nhid = 50\n",
    "\n",
    "word_nhid = 300\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "lr = 4\n",
    "\n",
    "aoptim = 'SGD'\n",
    "\n",
    "seed = 1111\n",
    "\n",
    "save = 'model.pt'\n",
    "\n",
    "weight = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")#torch.device代表将torch.Tensor分配到的设备的对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinesecorpus=open('../bishe_e1/data/mycorpus.txt','rt',encoding='utf-8').read()\n",
    "cs_list = list(Chinesecorpus)\n",
    "cs_set_list = list(set(cs_list))\n",
    "cs_set_list.sort(key = cs_list.index)\n",
    "corpus_char=''.join(cs_set_list)   \n",
    "charset=ChineseCharset(corpus_char)            ### 2192 +'<UNK>'+<PAD>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charset = Charset()#定义字符对象\n",
    "# vocab = Vocabulary()#定义词汇对象\n",
    "# vocab.load(\"data/myvocab.txt\")#读文件\n",
    "tag_set = Index()#定义对象\n",
    "tag_set.load(\"../data/my_tag2id.txt\")#读文件  标记-id\n",
    "relation_labels = Index()#定义对象\n",
    "relation_labels.load('../data/my_relation_labels.txt')#读文件   关系标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load('../data/my_train.pk')\n",
    "\n",
    "val_size = int(0.05 * len(train_data))#设置验证集大小\n",
    "train_data, val_data = random_split(train_data, [len(train_data)-val_size, val_size])##torch里的函数 随机切分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2091, 110)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分组排序 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(data, breakpoints):# 把数据集按照句子长度分组#data:包含三部分 句子中每个词的id 句子中每个词对应的char的id 句子对应的实体id\n",
    "    groups = [[] for _ in range(len(breakpoints)+1)]\n",
    "    for idx, item in enumerate(data):\n",
    "        i = bisect.bisect_left(breakpoints, len(item[0]))\n",
    "        groups[i].append(idx)\n",
    "    data_groups = [Subset(data, g) for g in groups]  #Subset是torch.utils.data.dataset中的\n",
    "    return data_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_groups = group(train_data, [50, 100, 150, 200, 250])\n",
    "val_data_groups = group(val_data, [50, 100, 150, 200, 250])    ####一个分组排序的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1266, 402, 65, 21, 95, 124, 210, 202, 4, 5, 6, 19, 20, 21, 688, 23, 23, 335, 186, 336, 325, 43, 44, 45, 46, 47, 48, 49, 50, 51, 9, 10, 118, 8, 56, 56, 56, 119, 52], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 27, 29, 0, 0, 0, 0, 26, 28, 28, 28, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(train_data_groups[0][1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采样与填充 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupBatchRandomSampler(object):##每一组的句子长度不同 分别建采样器进行随机采样\n",
    "    def __init__(self, data_groups, batch_size, drop_last):\n",
    "        self.batch_indices = []\n",
    "        for data_group in data_groups:\n",
    "            self.batch_indices.extend(list(BatchSampler(SubsetRandomSampler(data_group.indices),\n",
    "                                                        batch_size, drop_last=drop_last)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.batch_indices[i] for i in torch.randperm(len(self.batch_indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_indices, data):  #分batch   data有三部分   返回三部分的batch\n",
    "    #print(batch_indices,len(data))\n",
    "    batch = [data[idx] for idx in batch_indices]\n",
    "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, tags = zip(*sorted_batch)\n",
    "#     print([len(s) for s in sentences])\n",
    "\n",
    "    padded_sentences, lengths = pad_packed_sequence(pack_sequence([torch.LongTensor(_) for _ in sentences],enforce_sorted=False),  #文本长度对齐\n",
    "                                                    batch_first=True, padding_value=charset[\"<pad>\"])\n",
    "    padded_tags, _ = pad_packed_sequence(pack_sequence([torch.LongTensor(_) for _ in tags],enforce_sorted=False),\n",
    "                                         batch_first=True, padding_value=tag_set[\"O\"])\n",
    "\n",
    "    return padded_sentences.to(device),  padded_tags.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embedding = torch.Tensor(np.load('../data/char_embedding.npy'))\n",
    "char_embedding_size = char_embedding.size(1)\n",
    "\n",
    "\n",
    "# word_embeddings = torch.tensor(np.load(\"data/NYT_CoType/word2vec.vectors.npy\"))      #将词向量转换为张量 47463*300\n",
    "# word_embedding_size = word_embeddings.size(1)\n",
    "# pad_embedding = torch.empty(1, word_embedding_size).uniform_(-0.5, 0.5)  #随机初始化 1*word_embedding_size 大小的张量 取值在-0.5~0.5之间\n",
    "# unk_embedding = torch.empty(1, word_embedding_size).uniform_(-0.5, 0.5)\n",
    "# word_embeddings = torch.cat([pad_embedding, unk_embedding, word_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_embedding) == len(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = [1,2,3,4,5]\n",
    "sentences, targets, lengths = get_batch(batch_indices, train_data) #分batch\n",
    "\n",
    "# sentences[0]\n",
    "\n",
    "# charset[20],charset[21],charset[522],charset[23],charset[88]\n",
    "\n",
    "# charset['鹰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 276)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[4]),len(targets[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型搭建 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_channels = [char_embedding_size] + [char_nhid] * char_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (batch_size, seq_len, char_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, weight, channels, kernel_size, dropout, emb_dropout):  #weight 是word_embedding\n",
    "        super(CharEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weight, freeze=False)    ##加载词向量\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.conv_net = ConvNet(channels, kernel_size, dropout, dilated=True, residual=False)\n",
    "        #print(weight.size(0),weight.size(1))\n",
    "    def forward(self,  char_input):\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, embedding_size)\n",
    "        #  -> (batch_size, seq_len, embedding_size + char_features)\n",
    "        #  -> (batch_size, embedding_size + char_features, seq_len)\n",
    "        embeddings=self.embed(char_input)\n",
    "#         embeddings = torch.cat((embeddings, char_input), 2)                     ### 将字与词连接起来，以第2维度连接起来\n",
    "        embeddings=embeddings.transpose(1, 2).contiguous()                      ### 矩阵再次转置\n",
    "\n",
    "        #print(\"embeddings:----------\",embeddings.size())\n",
    "\n",
    "        # (batch_size, embedding_size + char_features, seq_len) -> (batch_size, conv_size, seq_len)\n",
    "        conv_out = self.conv_net(self.drop(embeddings))\n",
    "\n",
    "        # torch.cat(embeddings, conv_out), 1) ==>(batch_size, embedding_size + char_features, seq_len) -> (batch_size, conv_size + embedding_size + char_features, seq_len)\n",
    "        #  -> (batch_size, seq_len, conv_size + embedding_size + char_features)\n",
    "        return torch.cat((embeddings, conv_out), 1).transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_encoder = CharEncoder(char_embedding, word_channels, word_kernel_size,dropout=dropout, emb_dropout=emb_dropout)\n",
    "\n",
    "# sentences, targets, lengths = get_batch([0,1,2,3,4], train_data)\n",
    "\n",
    "# char_output = char_encoder(sentences)\n",
    "\n",
    "# len(char_output[0])  276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_dim,output_size,NUM_LAYERS):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_dim = hidden_dim      ###有\n",
    "        self.output_size=output_size      ### 分为多少类\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers = NUM_LAYERS)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, output_size)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.lstm.flatten_parameters()\n",
    "        #print(self.hidden_dim,self.input_size,self.output_size)\n",
    "        # self.hidden = self.init_hidden(inputs.size(1))\n",
    "        self.hidden = self.init_hidden(inputs.size(1))\n",
    "        lstm_out, self.hidden = self.lstm(inputs,self.hidden)\n",
    "        #lstm_out, self.hidden = self.lstm(inputs,None)\n",
    "        y = self.hidden2label(lstm_out)\n",
    "        return y\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.kaiming_uniform_(self.hidden2label.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)),\n",
    "                autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 weight, word_embedding_size, word_channels, word_kernel_size, num_tag, dropout, emb_dropout):    ##搭建网络结构\n",
    "        super(Model, self).__init__()\n",
    "#         self.char_encoder = CharEncoder(charset_size, char_embedding_size, char_channels, char_kernel_size,\n",
    "#                                         char_padding_idx, dropout=dropout, emb_dropout=emb_dropout)                ### 调用CharEncoder的init方法\n",
    "#         self.word_encoder = WordEncoder(weight, word_channels, word_kernel_size,\n",
    "#                                         dropout=dropout, emb_dropout=emb_dropout)\n",
    "        self.char_encoder = CharEncoder(char_embedding, word_channels, word_kernel_size,dropout=dropout, emb_dropout=emb_dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "#         self.char_conv_size = char_channels[-1]\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.word_conv_size = word_channels[-1]\n",
    "        #self.decoder = nn.Linear(self.char_conv_size+self.word_embedding_size+self.word_conv_size, num_tag)\n",
    "        self.decoder = Decoder(self.word_embedding_size+self.word_conv_size,    ###输入特征\n",
    "                               64 + self.word_embedding_size + self.word_conv_size,###隐藏层维度\n",
    "                               num_tag,NUM_LAYERS=1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, word_input):\n",
    "        batch_size = word_input.size(0)  #32\n",
    "        seq_len = word_input.size(1)    #句子长度\n",
    "#         char_input=char_input.contiguous()\n",
    "#         #print(char_input.view(-1, char_input.size(2)).size(0),char_input.view(-1, char_input.size(2)).size(1))##3200*10\n",
    "#         char_output = self.char_encoder(char_input.view(-1, char_input.size(2))).view(batch_size, seq_len, -1)#char_input.size(2)==20  调用forward方法\n",
    "        word_output = self.char_encoder(word_input)  ##(batch_size, seq_len, conv_size + embedding_size + char_features)\n",
    "        y = self.decoder(word_output)\n",
    "\n",
    "        return F.log_softmax(y, dim=2)\n",
    "#         return F.log_softmax(y, dim=1)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "# word_embeddings = torch.tensor(np.load(\"../data/char_embedding.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, targets, lengths = get_batch([0,1,2,3,4], train_data)\n",
    "# out = Model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model( weight=char_embedding,\n",
    "              word_embedding_size=char_embedding_size, word_channels=word_channels,\n",
    "              word_kernel_size=word_kernel_size, num_tag=len(tag_set), dropout=dropout,\n",
    "              emb_dropout=emb_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.1018, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.log_softmax(out,dim=2)[0][0][0]\n",
    "\n",
    "# a = torch.FloatTensor([\n",
    "#     [1.0,2,3],\n",
    "#     [1,1,1.0]\n",
    "# ])\n",
    "\n",
    "# F.log_softmax(a,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "weight = [weight] * len(tag_set)   #生成一维向量 初始化值为arg.weight 维度为len(tag_set)\n",
    "weight[tag_set[\"O\"]] = 1     #\"O\"标签对应的权值为1\n",
    "print(weight)\n",
    "\n",
    "# # .to(device)   #将weight转换为张量形式 device:分配到设备 这里是cpu\n",
    "# criterion = nn.NLLLoss(weight, size_average=False)      #定义的损失函数\n",
    "# optimizer = getattr(optim, optim)(model.parameters(), lr=args.lr)  #getattr 获取torch.optim对象的属性值 默认是SGD(随机梯度下降)   得到优化器的损失函数(模型参数)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor(weight).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IR LAB\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss(weight, size_average=False)      #定义的损失函数\n",
    "optimizer = getattr(optim, aoptim)(model.parameters(), lr=lr)  #getattr 获取torch.optim对象的属性值 默认是SGD(随机梯度下降)   得到优化器的损失函数(模型参数)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x166be268eb8>,\n",
       " <torch.utils.data.dataset.Subset at 0x166be268fd0>,\n",
       " <torch.utils.data.dataset.Subset at 0x166be268f98>,\n",
       " <torch.utils.data.dataset.Subset at 0x166be268e80>,\n",
       " <torch.utils.data.dataset.Subset at 0x166be268e10>,\n",
       " <torch.utils.data.dataset.Subset at 0x166be268128>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_groups.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(train_data_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()    #torch的模型训练\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    sampler = GroupBatchRandomSampler(train_data_groups, batch_size, drop_last=False)#采样器是定义的采样器对象 按组分batch分别采样\n",
    "\n",
    "    for idx, batch_indices in enumerate(sampler):\n",
    "        sentences, targets, lengths = get_batch(batch_indices, train_data) #分batch\n",
    "    #     print(lengths)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentences)  #词级别和字符级别建模\n",
    "        output = pack_padded_sequence(output, lengths, batch_first=True,enforce_sorted=False).data #文本长度对齐\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True,enforce_sorted=False).data #将标签的编码长度对齐\n",
    "        loss = criterion(output, targets)  #损失函数\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += len(targets)\n",
    "#         print('Hello world!!!')\n",
    "        if (idx+1) % log_interval == 0:\n",
    "            cur_loss = total_loss / count\n",
    "            elapsed = time.time() - start_time\n",
    "            percent = ((epoch-1)*len(sampler)+(idx+1))/(epochs*len(sampler))\n",
    "            remaining = elapsed / percent - elapsed\n",
    "            print(\"| Epoch {:2d}/{:2d} | Batch {:5d}/{:5d} | Elapsed Time {:s} | Remaining Time {:s} | \"\n",
    "                  \"lr {:4.2e} | Loss {:5.3f} |\".format(epoch, epochs, idx+1, len(sampler), time_display(elapsed),\n",
    "                                                       time_display(remaining), lr, cur_loss))\n",
    "            total_loss = 0\n",
    "            count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_groups):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0.000000001\n",
    "    TP = 0.000000001\n",
    "    TP_FP = 0.000000001\n",
    "    TP_FN = 0.000000001\n",
    "    with torch.no_grad():\n",
    "        for batch_indices in GroupBatchRandomSampler(data_groups, batch_size, drop_last=False):\n",
    "            sentences,targets, lengths = get_batch(batch_indices, train_data)\n",
    "            output = model(sentences)\n",
    "            tp, tp_fp, tp_fn = measure(output, targets, lengths,sentences)\n",
    "            TP += tp\n",
    "            TP_FP += tp_fp\n",
    "            TP_FN += tp_fn\n",
    "            output = pack_padded_sequence(output, lengths, batch_first=True).data\n",
    "            targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            count += len(targets)\n",
    "            print(count,TP_FP,TP_FN)\n",
    "    return total_loss / count, TP/TP_FP, TP/TP_FN, 2*TP/(TP_FP+TP_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure(output, targets, lengths,sentences)\n",
    "def measure(output, targets, lengths,sentences):\n",
    "    assert output.size(0) == targets.size(0) and targets.size(0) == lengths.size(0)\n",
    "    tp = 0\n",
    "    tp_fp = 0\n",
    "    tp_fn = 0\n",
    "    batch_size = output.size(0)\n",
    "    output = torch.argmax(output, dim=-1)\n",
    "    for i in range(batch_size):\n",
    "        sentence = sentences[i]\n",
    "        length = lengths[i]\n",
    "        out = output[i][:length].tolist()\n",
    "        target = targets[i][:length].tolist()\n",
    "        out_triplets = song_get_triplets(out,sentence)\n",
    "        print(\"out:\",out_triplets)\n",
    "        tp_fp += len(out_triplets)\n",
    "        target_triplets = song_get_triplets(target,sentence)\n",
    "        print(\"target:\",target_triplets)\n",
    "        tp_fn += len(target_triplets)\n",
    "        for target_triplet in target_triplets:\n",
    "            for out_triplet in out_triplets:\n",
    "                if out_triplet == target_triplet:\n",
    "                    tp += 1\n",
    "    return tp, tp_fp, tp_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_get_triplets(tags,outset):\n",
    "    outsent = charset\n",
    "#     outsent = out[0]\n",
    "#     tags = out[1]\n",
    "#     assert len(outsent) == len(tags)\n",
    "    temp = {}\n",
    "    triplets = []\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if tag == tag_set[\"O\"]:\n",
    "            continue\n",
    "    #     entity=[]\n",
    "        pos, relation_label, role = tag_set[tag].split(\"-\")\n",
    "        if pos == \"B\" or pos == \"S\":\n",
    "            entity=[]\n",
    "            entity.append(idx)\n",
    "        if pos==\"I\" or pos==\"E\":\n",
    "            try:\n",
    "                entity.append(idx)\n",
    "            except UnboundLocalError:\n",
    "                entity = []\n",
    "                entity.append(idx)\n",
    "        if pos==\"S\" or pos==\"E\":\n",
    "            if relation_label not in temp:\n",
    "                temp[relation_label] = [[], []]\n",
    "            temp[relation_label][int(role) - 1].append(entity)\n",
    "\n",
    "    label = list(temp.keys())[0]\n",
    "#     print(label)\n",
    "    entitys = temp[label]\n",
    "#     print('entitys:',entitys)\n",
    "    entity_tmp = entitys[0]\n",
    "    print(entity_tmp)\n",
    "    if len(entity_tmp) == 0:\n",
    "        entity1 = []\n",
    "    else:\n",
    "        entity1 = entity_tmp[0]\n",
    "        \n",
    "    entity_tmp = entitys[1]\n",
    "   \n",
    "    if len(entity_tmp) == 0:\n",
    "        entity2 = []\n",
    "    else:\n",
    "        entity2 = entity_tmp[0]\n",
    "#     entity2 = entitys[1][0]\n",
    "#     print(entity1,entity2)\n",
    "\n",
    "    entity1_word = []\n",
    "    for i in entity1:\n",
    "        entity1_word.append(outsent[i])\n",
    "    \n",
    "    entity1_word = ''.join(entity1_word)\n",
    "#     print(entity1_word)\n",
    "\n",
    "    entity2_word = []\n",
    "    for i in entity2:\n",
    "        entity2_word.append(outsent[i])\n",
    "   \n",
    "    entity2_word = ''.join(entity2_word)\n",
    "#     print(entity2_word)\n",
    "    \n",
    "    triplets.append(entity1_word)\n",
    "    triplets.append(label)\n",
    "    triplets.append(entity2_word)\n",
    "    \n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------\n",
      "[]\n",
      "out: ['', 'traffic_in', '包可疑']\n",
      "[[70, 71]]\n",
      "\n",
      "\n",
      "target: ['潭村', 'sell_drugs_to', '称量扣']\n",
      "[[5, 6, 7], [211, 212, 213]]\n",
      "\n",
      "\n",
      "\n",
      "out: ['控，2', 'provide_shelter_for', '苯丙胺']\n",
      "[[55, 56, 57]]\n",
      "\n",
      "\n",
      "\n",
      "target: ['63日', 'provide_shelter_for', '抓获二']\n",
      "[[30, 31, 32, 47], [218, 219, 220, 221, 222, 236, 287, 288, 295], [218, 219, 220, 221, 222, 236, 287, 288, 295]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "out: ['张伍在胺', 'traffic_in', '01']\n",
      "[[48, 49, 50]]\n",
      "\n",
      "\n",
      "\n",
      "target: ['（冰毒', 'sell_drugs_to', '以烫']\n",
      "[[10, 11, 15]]\n",
      "\n",
      "\n",
      "\n",
      "out: ['5年一', 'provide_shelter_for', '）。\\n']\n",
      "[[184, 185]]\n",
      "\n",
      "\n",
      "target: ['国刑', 'sell_drugs_to', '比例混']\n",
      "[[24, 25, 26], [231, 233]]\n",
      "\n",
      "\n",
      "\n",
      "out: ['容留陈', 'provide_shelter_for', '理化检']\n",
      "[[24, 25, 26]]\n",
      "\n",
      "\n",
      "\n",
      "target: ['容留陈', 'provide_shelter_for', '理化检']\n",
      "[]\n",
      "out: ['', 'provide_shelter_for', '简某容']\n",
      "[[32, 33, 34]]\n",
      "\n",
      "\n",
      "\n",
      "target: ['在家楼', 'provide_shelter_for', '水泥路']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'entity' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-f5e0574de17f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#评估的是验证集\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-2b404932aad8>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(data_groups)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp_fp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mTP\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mTP_FP\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtp_fp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-ef95c9522aa0>\u001b[0m in \u001b[0;36mmeasure\u001b[1;34m(output, targets, lengths, sentences)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout_triplets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong_get_triplets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"out:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_triplets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtp_fp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_triplets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-114-8297ea82b567>\u001b[0m in \u001b[0;36msong_get_triplets\u001b[1;34m(tags, outset)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"I\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"E\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"S\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"E\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrelation_label\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'entity' referenced before assignment"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "lr = lr\n",
    "all_val_loss = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_f1 = []\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    print(\"-\" * 118)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train()\n",
    "        val_loss, precision, recall, f1 = evaluate(val_data_groups) #评估的是验证集\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\"-\" * 118)\n",
    "        print(\"| End of Epoch {:2d} | Elapsed Time {:s} | Validation Loss {:5.3f} | Precision {:5.3f} \"\n",
    "              \"| Recall {:5.3f} | F1 {:5.3f} |\".format(epoch, time_display(elapsed),\n",
    "                                                       val_loss, precision, recall, f1))\n",
    "        print(\"-\" * 118)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr = lr / 4.0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        all_f1.append(f1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 118)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "\n",
    "with open(\"record-batchsize64-epochs30-lr4.tsv\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    for idx in range(len(all_val_loss)):\n",
    "        f.write(\"{:d}\\t{:5.3f}\\t{:5.3f}\\t{:5.3f}\\t{:5.3f}\\n\"\n",
    "                .format(idx+1, all_val_loss[idx], all_precision[idx], all_recall[idx], all_f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
