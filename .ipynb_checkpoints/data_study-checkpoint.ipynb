{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***train.pk文件***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/my_train.pk', 'rb') as f1:\n",
    "    train_data = pickle.load(f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) ### 2098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0]) ###tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]) ### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 45为句子中的词的数量\n",
    "### sentence_word_idx, tokens_idx, tags_idx\n",
    "len(train_data[0][0]) ### 45\n",
    "len(train_data[0][1]) ### 45\n",
    "len(train_data[0][2]) ###45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1540, 551, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [818, 463, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [194, 1481, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [777, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1836, 277, 216, 1305, 1429, 2192, 2192, 2192, 2192, 2192],\n",
       " [216, 216, 472, 1061, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1946, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1324, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1791, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [376, 1968, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [777, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1819, 1408, 1058, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1007, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [900, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1109, 427, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1681, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [716, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1165, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [716, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [772, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [900, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [716, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1995, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [716, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [393, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [900, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [2091, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1007, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [900, 900, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [607, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1324, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1502, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1771, 774, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [777, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [739, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [473, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [502, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1946, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [644, 339, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [502, 1355, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [1334, 144, 1483, 2076, 927, 2192, 2192, 2192, 2192, 2192],\n",
       " [698, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [2034, 1281, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [2006, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192],\n",
       " [553, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192, 2192]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1] ### 每一行都表示token_idx（每个词的字序列的id） 2192表示填充字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以暂时确定出，train.pk文件中所包含的数字是将文本序列化以后的结果<br>\n",
    "但是，是如何处理的呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***train.json文件***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "案件总数： 2201\n",
      "Counter({'provide_shelter_for': 835, 'sell_drugs_to': 472, 'posess': 470, 'traffic_in': 424})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "with open('../../data/mytrain.json','r',encoding='gbk') as f2:\n",
    "    lines = f2.readlines()\n",
    "#     raw_tra = json.load(f2)\n",
    "f2.close()\n",
    "\n",
    "length = len(lines)\n",
    "\n",
    "label_list = []\n",
    "for i in range(length):\n",
    "    sentence = json.loads(lines[i])\n",
    "    label = sentence['relationMentions'][0]['label']\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "# sentence = json.loads(lines[10])\n",
    "print('案件总数：',length)\n",
    "assert len(label_list) == length\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relationMentions': [{'em1Text': '简某某',\n",
       "   'e1start': 25,\n",
       "   'e21start': 47,\n",
       "   'label': 'provide_shelter_for',\n",
       "   'em2Text': '简某某'}],\n",
       " 'articleId': 20,\n",
       " 'sentId': 91,\n",
       " 'entityMentions': [{'start': 7,\n",
       "   'text': '2015年11月份的一天晚上',\n",
       "   'end': 21,\n",
       "   'label': 'NT'},\n",
       "  {'start': 25, 'text': '简某某', 'end': 28, 'label': 'Nh'},\n",
       "  {'start': 30, 'text': '陈某', 'end': 32, 'label': 'Nh'},\n",
       "  {'start': 33, 'text': '吴某', 'end': 35, 'label': 'Nh'},\n",
       "  {'start': 36, 'text': '杨某某', 'end': 39, 'label': 'Nh'},\n",
       "  {'start': 40, 'text': '张某', 'end': 42, 'label': 'Nh'},\n",
       "  {'start': 43, 'text': '伍某某', 'end': 46, 'label': 'Nh'},\n",
       "  {'start': 47, 'text': '简某某', 'end': 50, 'label': 'Nh'},\n",
       "  {'start': 64, 'text': '甲基苯丙胺', 'end': 69, 'label': 'NDR'},\n",
       "  {'start': 70, 'text': '冰毒', 'end': 72, 'label': 'NDR'}],\n",
       " 'sentText': '公诉机关指控，2015年11月份的一天晚上，被告人简某某容留陈某、吴某、杨某某、张某、伍某某在简某某家一楼卧室，以烫吸的方式吸食甲基苯丙胺（冰毒）。'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test.json文件 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "案件总数： 550\n",
      "Counter({'provide_shelter_for': 208, 'posess': 119, 'traffic_in': 114, 'sell_drugs_to': 109})\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/mytest.json','r',encoding='gbk') as f2:\n",
    "    lines = f2.readlines()\n",
    "#     raw_tra = json.load(f2)\n",
    "f2.close()\n",
    "\n",
    "length = len(lines)\n",
    "\n",
    "label_list = []\n",
    "for i in range(length):\n",
    "    sentence = json.loads(lines[i])\n",
    "    label = sentence['relationMentions'][0]['label']\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "# sentence = json.loads(lines[10])\n",
    "print('案件总数：',length)\n",
    "assert len(label_list) == length\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/mytrain.json','r',encoding='gbk') as f2:\n",
    "#     lines = f2.readlines()\n",
    "    \n",
    "# f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relationMentions': [{'em1Text': '覃某',\n",
       "   'e1start': 100,\n",
       "   'e21start': 110,\n",
       "   'label': 'sell_drugs_to',\n",
       "   'em2Text': '黄某'}],\n",
       " 'articleId': 24,\n",
       " 'sentId': 102,\n",
       " 'entityMentions': [{'start': 10, 'text': '黄某', 'end': 12, 'label': 'Nh'},\n",
       "  {'start': 17, 'text': '覃某', 'end': 19, 'label': 'Nh'},\n",
       "  {'start': 56, 'text': '2014年12月', 'end': 64, 'label': 'NT'},\n",
       "  {'start': 90, 'text': '2015年10月', 'end': 98, 'label': 'NT'},\n",
       "  {'start': 100, 'text': '覃某', 'end': 102, 'label': 'Nh'},\n",
       "  {'start': 110, 'text': '黄某', 'end': 112, 'label': 'Nh'},\n",
       "  {'start': 121, 'text': '覃某', 'end': 123, 'label': 'Nh'},\n",
       "  {'start': 124, 'text': '邓某', 'end': 126, 'label': 'Nh'}],\n",
       " 'sentText': '公诉机关指控，被告人黄某与吸毒人员覃某（因吸食毒品已被松滋公安局行政拘留并决定强制隔离戒毒）曾多次一起吸毒，且于2014年12月被松滋市公安局查获，但此后仍不思悔改，继续吸食毒品。2015年10月，由覃某提供毒品，被告人黄某先后3次默许并参与覃某、邓某（因吸食毒品已被松滋市公安局行政拘留并决定社区戒毒）等人在其住所吸食毒品。具体事实如下：'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mydatahandle.py ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['甲', '基', '苯', '丙', '胺']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('甲基苯丙胺')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
